{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extension By flip: Executing!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'all_points_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a301d2954495>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a301d2954495>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#                   iaa.Sequential([iaa.Affine(rotate=-10)]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Gaussian Blur (three levels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdata_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"GaussianBlur_low\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miaa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miaa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mdata_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"GaussianBlur_mid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miaa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miaa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mdata_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"GaussianBlur_high\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miaa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miaa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a301d2954495>\u001b[0m in \u001b[0;36mdata_augmentation\u001b[1;34m(dataset_dir_old, dataset_dir_new_prefix, iaa_name, seq)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mkey_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'all_points_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'all_points_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'all_points_x'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to verify all examples in the readme.\n",
    "Simply execute\n",
    "    python test_readme_examples.py\n",
    "\"\"\"\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "datasets_path = 'C:/Users/CES/[JupyterProject]/augmentation/hellimage/'\n",
    "\n",
    "\n",
    "def main():\n",
    "    old = \"C:/Users/CES/[JupyterProject]/augmentation/hellimage/Old\"\n",
    "    new = \"C:/Users/CES/[JupyterProject]/augmentation/hellimage/New\"\n",
    "    # # Rotate 10 degrees\n",
    "    #data_augmentation(old, new, \"rotate_300\", iaa.Sequential([iaa.Affine(rotate=-10)]))\n",
    "    # # -10 degrees\n",
    "    # data_augmentation(old,\n",
    "    #                   new, \"rotate_-10\",\n",
    "    #                   iaa.Sequential([iaa.Affine(rotate=-10)]))\n",
    "    # Gaussian Blur (three levels)\n",
    "    data_augmentation(old, new, \"GaussianBlur_low\", iaa.Sequential([iaa.GaussianBlur(sigma=1)]))\n",
    "    data_augmentation(old, new, \"GaussianBlur_mid\", iaa.Sequential([iaa.GaussianBlur(sigma=2)]))\n",
    "    data_augmentation(old, new, \"GaussianBlur_high\",iaa.Sequential([iaa.GaussianBlur(sigma=3)]))\n",
    "    # # ( )\n",
    "    data_augmentation(old, new, \"AdditiveGaussianNoise_5\", iaa.Sequential([iaa.AdditiveGaussianNoise(scale=5)]))\n",
    "    data_augmentation(old, new, \"AdditiveGaussianNoise_10\", iaa.Sequential([iaa.AdditiveGaussianNoise(scale=10)]))\n",
    "    # # Brightness change\n",
    "    data_augmentation(old, new, \"light_1.15\", iaa.Sequential([iaa.Multiply(mul=1.15)]))\n",
    "    data_augmentation(old, new, \"light_1.3\", iaa.Sequential([iaa.Multiply(mul=1.3)]))\n",
    "    data_augmentation(old, new, \"light_0.85\", iaa.Sequential([iaa.Multiply(mul=0.85)]))\n",
    "    data_augmentation(old, new, \"light_0.7\", iaa.Sequential([iaa.Multiply(mul=0.7)]))\n",
    "    # # \n",
    "    #data_augmentation(old, new, \"Affine_scale_1.5\",iaa.Sequential([iaa.Affine(scale={\"x\": 1.5, \"y\": 1.5})]))\n",
    "    # data_augmentation(old,\n",
    "    #                   new, \"Affine_scale_0.8\",\n",
    "    #                   iaa.Sequential([iaa.Affine(scale={\"x\": 0.8, \"y\": 0.8})]))\n",
    "\n",
    "    # # \n",
    "    # data_augmentation(old,\n",
    "    #                   new, \"Affine_xy_20\",\n",
    "    #                   iaa.Sequential([iaa.Affine(translate_px={\"x\": 20, \"y\": 20})]))\n",
    "    # data_augmentation(old,\n",
    "    #                   new, \"Affine_x_10\",\n",
    "    #                   iaa.Sequential([iaa.Affine(translate_px={\"x\": 10, \"y\": 0})]))\n",
    "\n",
    "    ################################################################################ ##############################################################\n",
    "    merge_data('C:/Users/CES/[JupyterProject]/augmentation/hellimage/PreMerger')\n",
    "    ############################################################################### ###############################################################\n",
    "\t# # Mirror\n",
    "#    flip_all(\"All files in the directory are mirrored\")\n",
    "\n",
    "def flip_all(datasets_path):\n",
    "    #All files in the source directory\n",
    "    datasets_path_children = os.listdir(datasets_path)\n",
    "    for temp in datasets_path_children:\n",
    "        data_augmentation(os.path.join(datasets_path, temp),\n",
    "                          os.path.join(datasets_path, temp), \"_flip\",\n",
    "                          iaa.Sequential([iaa.Fliplr(1)]))\n",
    "\n",
    "\n",
    "def merge_data(datasets_path, new_dataset):\n",
    "    # Empty catalog collection\n",
    "    datasets = []\n",
    "    #All files in the source directory\n",
    "    datasets_path_children = os.listdir(datasets_path)\n",
    "    # \n",
    "    for datasets_path_child in datasets_path_children:\n",
    "        tmp_path = os.path.join(datasets_path, datasets_path_child)\n",
    "        if os.path.isdir(tmp_path):\n",
    "            datasets.append(str(tmp_path))\n",
    "    # Create a new directory\n",
    "    if not os.path.exists(new_dataset):\n",
    "        os.makedirs(new_dataset)\n",
    "    ################################################################################################# ########################################################################\n",
    "    # \n",
    "    annotations = {}\n",
    "    # merge annotation\n",
    "    for dataset in datasets:\n",
    "        annotation = json.load(open(os.path.join(dataset, \"Hell_aug.json\")))\n",
    "        annotations.update(annotation)\n",
    "    # Write the annotation file\n",
    "    with open(os.path.join(new_dataset, \"Hell_aug.json\"), 'w') as f:\n",
    "        json.dump(annotations, f)\n",
    "    ######################################################################################### ########################################################################\n",
    "    for dataset in datasets:\n",
    "        files = os.listdir(dataset)\n",
    "        for file in files:\n",
    "            # Copy only pictures\n",
    "            if not file.endswith('json'):\n",
    "                shutil.copyfile(os.path.join(dataset, file), os.path.join(new_dataset, file))\n",
    "\n",
    "\n",
    "def data_augmentation(dataset_dir_old, dataset_dir_new_prefix, iaa_name, seq):\n",
    "    print(\"Data Extension By flip: Executing!\")\n",
    "    #determine the law of transformation\n",
    "    seq_det = seq.to_deterministic()\n",
    "    # Determine if the folder exists, if not, create it\n",
    "    dataset_dir_new = dataset_dir_new_prefix + iaa_name\n",
    "    if os.path.exists(dataset_dir_new).__eq__(False):\n",
    "        os.makedirs(dataset_dir_new)\n",
    "    \n",
    "    #Loading callout information\n",
    "    annotations = json.load(open(os.path.join(dataset_dir_old, \"Hell_aug.json\")))\n",
    "    annotations_new = copy.deepcopy(annotations)\n",
    "    annotations_new_keys = []\n",
    "    #Get the key-value pair (old)\n",
    "    for key in annotations_new:\n",
    "        annotations_new_keys.append(key)\n",
    "\n",
    "    # Don't have the outermost key, the inner layer is List\n",
    "    annotations_values = list(annotations.values())\n",
    "    # Determine if there is a Regions property and build a new List\n",
    "    annotations_values = [a for a in annotations_values if a['regions']]\n",
    "    \n",
    "\n",
    "    #  \n",
    "    for i, (annotations_value) in enumerate(annotations_values):\n",
    "        # corresponding key points\n",
    "        key_points_old = []\n",
    "        if type(annotations_value['regions']) is dict:\n",
    "            polygons = [r['shape_attributes'] for r in annotations_value['regions'].values()]\n",
    "        else:\n",
    "            polygons = [r['shape_attributes'] for r in annotations_value['regions']]\n",
    "        # \n",
    "        filename = annotations_value['filename']\n",
    "        image_old = Image.open(os.path.join(dataset_dir_old, filename))\n",
    "        image_old = np.array(image_old)\n",
    "        # polygons List, including a map of multiple Regions\n",
    "        for j, (b) in enumerate(polygons):\n",
    "            # Increase the key point of the picture\n",
    "            key_points = []\n",
    "            \n",
    "            print(b['all_points_x'])\n",
    "            \n",
    "            for k in range(0, len(b['all_points_x'])):\n",
    "                try:\n",
    "                    x_old = annotations_new[annotations_new_keys[i]]['regions'][j]['shape_attributes']['all_points_x'][\n",
    "                        k]\n",
    "                    y_old = annotations_new[annotations_new_keys[i]]['regions'][j]['shape_attributes']['all_points_y'][\n",
    "                        k]\n",
    "                    x = b['all_points_x'][k]\n",
    "                    y = b['all_points_y'][k]\n",
    "                    # print('old:(%d,%d) new(%d,%d)' % (x_old, y_old, x, y))\n",
    "                    key_points.append(ia.Keypoint(x=x, y=y))\n",
    "                    \n",
    "                except IndexError:\n",
    "                    print(\"Error: i:\" + str(i) + \" name:\" + annotations_new_keys[i] + \" j:\" + str(j) + \" k:\" + str(k))\n",
    "            key_points_old.append(ia.KeypointsOnImage(key_points, shape=image_old.shape))\n",
    "        # \n",
    "        image_new = seq_det.augment_image(image_old)\n",
    "        #Keypoint transformation, is a List, multiple Region\n",
    "        key_points_new = seq_det.augment_keypoints(key_points_old)\n",
    "\n",
    "        # \n",
    "        image_file_name = filename.replace(\".png\", \"_\" + iaa_name + \".png\")\n",
    "        image_path_new = os.path.join(dataset_dir_new, image_file_name)\n",
    "        #Save new image\n",
    "        image_new = Image.fromarray(image_new.astype('uint8')).convert('RGB')\n",
    "        image_new.save(image_path_new, \"PNG\")\n",
    "        # Get the file size first\n",
    "        image_size = os.path.getsize(image_path_new)\n",
    "        # Replace Json's Key\n",
    "        annotations_new.update({image_file_name + str(image_size): annotations_new.pop(annotations_new_keys[i])})\n",
    "        #Updated Key\n",
    "        annotations_new_keys[i] = image_file_name + str(image_size)\n",
    "        #update filename\n",
    "        annotations_new[annotations_new_keys[i]]['filename'] = image_file_name\n",
    "        # update size\n",
    "        annotations_new[annotations_new_keys[i]]['size'] = image_size\n",
    "\n",
    "        # traverse the transformed point set (new), the same number as the old point, where idx is equivalent to j above\n",
    "        for j in range(0, len(key_points_new)):\n",
    "            for k, (key_point) in enumerate(key_points_new[j].keypoints):\n",
    "                x_old = annotations_new[annotations_new_keys[i]]['regions'][j]['shape_attributes']['all_points_x'][k]\n",
    "                y_old = annotations_new[annotations_new_keys[i]]['regions'][j]['shape_attributes']['all_points_y'][k]\n",
    "                x_new = key_point.x\n",
    "                y_new = key_point.y\n",
    "                annotations_new[annotations_new_keys[i]]['regions'][j]['shape_attributes']['all_points_x'][k] = x_new\n",
    "                annotations_new[annotations_new_keys[i]]['regions'][j]['shape_attributes']['all_points_y'][k] = y_new\n",
    "        # # \n",
    "        #     image_old = key_points_old[j].draw_on_image(image_old)\n",
    "        # # \n",
    "        #     image_new = key_points_new[j].draw_on_image(image_new)\n",
    "        # #  \n",
    "        # ia.imshow(np.concatenate((image_old, image_new), axis=1))\n",
    "    # print(type(annotations_new))\n",
    "    with open(os.path.join(dataset_dir_new, \"Hell_aug.json\"), 'w') as f:\n",
    "        json.dump(annotations_new, f)\n",
    "    print('Data extension By flip: Done! ')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
